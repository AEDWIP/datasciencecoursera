---
title: "Practical Machine Learning Course Project"
author: "Andrew E. Davidson"
date: "September 25, 2015"
output: html_document
---

## Synopsis
aedwip

ref: http://topepo.github.io/caret/preprocess.html

## Data processing

Down load the data and documentation
```{r load training and testing data }
fileURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
fileName <- "pml-training.csv"
dataDir <- "data/"
trainingDataFile <- sprintf("%s%s", dataDir, fileName)
if (!file.exists(dataDir)) { dir.create(dataDir)}
if (!file.exists(trainingDataFile)) {
  download.file(fileURL, destfile=trainingDataFile, method="curl")
  dateDownLoaded <- date()
  write(dateDownLoaded, file=sprintf("%s%s", dataDir, "dateDownLoaded.txt"))
}

fileURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
fileName <- "pml-testing.csv"
testingDataFile <- sprintf("%s%s", dataDir, fileName)
if (!file.exists(testingDataFile)) {
  download.file(fileURL, destfile=testingDataFile, method="curl")
}
```

Down load the data releated documentation if needed
```{r down load documentation}
fileURL <- "http://groupware.les.inf.puc-rio.br/har"
fileName <- "har.html"
docFile <- sprintf("%s%s", dataDir, fileName)
if (!file.exists(docFile)) {
  download.file(fileURL, destfile=docFile, method="curl")
}
```

Load data into memory and remove tempory variables. We'll temporarly turn off warnings about missing objects not found

```{r load data into memory}
library(data.table)
if (!exists("rawTrainTable")){
    csv <- read.csv(trainingDataFile, header=TRUE, fill=TRUE)
    rawTrainTable <- data.table(csv)
}

if (!exists("rawTestTable")){
    csv <- read.csv(testingDataFile, header=TRUE, fill=TRUE)
    rawTestTable <- data.table(csv)
}
options(warn=-1) 
rm(csv, dataDir, dataFileZip, dateDownLoaded, testingDataFile,trainingDataFile, fileURL, docFile, fileName)
options(warn=0) 
```

## Pre processing and selecting data
We have a lot of data. There is a good chance a lot of it is not useful. We  want to reduce the number of featues we use if possible. Our model will train faster and be easier to understand.In general we do not want to edit the raw data we down loaded. Instead we create a new clean data set. 

The first thing we want to get rid of the first 7 columns.
"X" appears to identify each observation. If we need to know who the subject
our model will not generalize well. The other variable have to do with time stamp
and 'binning' samples based on time. If our model does not work well we can
consider using them in a future analysis.

```{r pre processing and selcting data remove first 7 columns}
rmCols <- names(rawTrainTable)[1:7]
rmCols
t1 <- rawTrainTable[,!rmCols, with=FALSE]
rm(rmCols)
```

Next we want to find and remove any features that are missing most of their data
```{r remove features with lots of missing data}
n <- nrow(t1) 
lotsOfNA <- t1[, lapply(.SD, function(x) (sum(is.na(x), na.rm=TRUE) / n)) > 0.5]
rmColNames <- names(t1)[lotsOfNA]
t2 <- t1[,!rmColNames, with=FALSE]
rm(t1, lotsOfNA, rmColNames, n)
```

Check to see if any of the factors haveZero- and Near Zero-Variance. If remove
them, they will not be good predictors

```{r look for near zero variance factors}
library(caret)
nzv <- nearZeroVar(t2, saveMetrics= TRUE)
nearZeroCols <- nzv[nzv$nzv,4]
numberOfNearZeroFeatures <- sum(nearZeroCols)
rmColNames <- names(t2)[nzv[,4]]
t3 <- t2[,!rmColNames, with=FALSE]
rm(t2, nearZeroCols, numberOfNearZeroFeatures, rmColNames, nzv)
```

Look for pairs of highly correlated variables. Removing one of the variables
will make our model simpler and should train faster.

```{r look for highly correlated variables} 
colNames <- grep("classe", names(t3), invert=TRUE)
tmp <- t3[,colNames, with=FALSE]
descrCor <- cor(tmp)
summary(descrCor[upper.tri(descrCor)])

highlyCorDescr <- findCorrelation(descrCor, cutoff = .90)
t4 <- t3[,!highlyCorDescr, with=FALSE]
rm(tmp, t3, descrCor, highlyCorDescr, colNames)
```

##Random forest
our label is 'classe'
http://topepo.github.io/caret/Random_Forest.html

Tuning Parameters: mtry (#Randomly Selected Predictors)

http://bigcomputing.blogspot.com/2014/10/an-example-of-using-random-forest-in.html

parameter ntree = integer # what is default value? 

caret train parmeter preProcess="pca"

```{r train model}
library(caret)
library(doParallel)
set.seed(32343)
#rpart is regression classifier
#modFit <- train(classe ~ .,method="rpart",data=trainTable)
#print(modFit$finalModel)

# rf is random forest
#system.time(modFit <- train(classe ~ .,data=t4, method="rf", proximity=TRUE, allowParallel=TRUE))

# speed up random forest by turning off resampling
fitControl <- trainControl(method = "none")
tgrid <- expand.grid(mtry=c(6)) # mtry (#Randomly Selected Predictors)

system.time(model  <- train(classe ~ .,
                            data = t4, 
                            method="rf", 
                            trControl=fitControl,
                            tuneGrid = tgrid, 
                            allowParallel=TRUE))

model$finalModel
#saveRDS(modFit, "mymodel.rds")
# mod2 <- readRDS("mymodel.rds")
```

## How well did we do?

```{r test data}
# running train changed the name of column "classe" to ".outcome"
setnames(t4, ".outcome", "classe")
#testTable <- rawTestTable[, names(t4), with=FALSE]
#t <- match(names(t4), names(rawTestTable))

# test data does not have a column 'classe'
l <- names(t4) != "classe"
colNames <- names(t4)[l]
testTable <- rawTestTable[, colNames, with=FALSE]

```

